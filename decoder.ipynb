{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqoluknJZFxw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, seq_length=32, vocab_size=30000, pad_token_id=0):\n",
        "        self.samples = []\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_length = seq_length\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            actual_length = torch.randint(5, seq_length + 1, (1,)).item()\n",
        "            tokens = torch.randint(5, vocab_size, (actual_length,))\n",
        "            padding = torch.full((seq_length - actual_length,), pad_token_id)\n",
        "            input_ids = torch.cat([tokens, padding], dim=0)\n",
        "\n",
        "            attention_mask = torch.cat([\n",
        "                torch.ones(actual_length),\n",
        "                torch.zeros(seq_length - actual_length)\n",
        "            ], dim=0)\n",
        "\n",
        "            self.samples.append({\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.samples[idx]['input_ids'].long(),\n",
        "            'attention_mask': self.samples[idx]['attention_mask'].long()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "We4PVcYH4gfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "dummy_dataset = DummyDataset(num_samples=20, seq_length=32, vocab_size=30000, pad_token_id=0)\n",
        "dataloader = DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "TIaqZJn54iFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_sequence_length, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.token_embeds = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.position_embeds = nn.Embedding(max_sequence_length, hidden_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    B, L = input_ids.shape\n",
        "    token_embeddings = self.token_embeds(input_ids)\n",
        "\n",
        "    position_ids = torch.arange(L,device = input_ids.device).unsqueeze(0).expand(B,-1)\n",
        "    position_embeddings = self.position_embeds(position_ids)\n",
        "\n",
        "    embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "    embeddings = self.layernorm(embeddings)\n",
        "\n",
        "    embeddings = self.dropout(embeddings)\n",
        "\n",
        "    return embeddings #B x L x H"
      ],
      "metadata": {
        "id": "GAgdHX_ZZN8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding_layer():\n",
        "    vocab_size = 100\n",
        "    hidden_size = 64\n",
        "    max_len = 32\n",
        "    batch_size = 2\n",
        "    seq_len = 16\n",
        "\n",
        "    embedding = Embedding(vocab_size, hidden_size, max_len)\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    output = embedding(input_ids)\n",
        "    assert output.shape == (batch_size, seq_len, hidden_size), \"Wrong shape!\"\n",
        "    print(\"✅ Embedding layer test passed!\")"
      ],
      "metadata": {
        "id": "6qCqgwhiaCQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_embedding_layer()"
      ],
      "metadata": {
        "id": "aLa237xzaDlv",
        "outputId": "d50063d1-c596-4d13-b6ad-cbee7ee35642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedding layer test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, num_heads, hidden_size, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    assert hidden_size % num_heads == 0\n",
        "    self.query = nn.Linear(hidden_size, hidden_size)\n",
        "    self.key = nn.Linear(hidden_size, hidden_size)\n",
        "    self.value = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.head_dim = hidden_size // num_heads\n",
        "\n",
        "    self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "  def forward(self, x, attention_mask = None):\n",
        "    #masked multihead self attention. so causal masking is true\n",
        "    #input comes from the embedding layer.\n",
        "    #shape = b x l x h\n",
        "    B, L, H = x.shape\n",
        "\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "\n",
        "    Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    K = K.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    V = V.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "    #so shape = B x Heads X L x Head_Dim\n",
        "    scores = torch.matmul(Q, K.transpose(-1,-2)) / self.scale # B x Heads x L x L\n",
        "\n",
        "    causal_mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
        "    scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "    # Apply padding mask if provided (for ignoring pad tokens)\n",
        "    if attention_mask is not None:\n",
        "        # mask should be [B, 1, 1, L] shape for broadcasting.\n",
        "        scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    attention_output = torch.matmul(attention_weights, V)  # B x Heads x L_q x Head_dim\n",
        "    attention_output = attention_output.transpose(1, 2).contiguous().view(B, L, H)\n",
        "\n",
        "    output = self.out_proj(attention_output)\n",
        "\n",
        "    return output #B, L, H\n"
      ],
      "metadata": {
        "id": "PrZqqvtebC1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, num_heads, hidden_size, intermediate_size, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadSelfAttention(num_heads, hidden_size, dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm1 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(hidden_size,intermediate_size),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(intermediate_size,hidden_size),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, attention_mask = None):\n",
        "    attention_output = self.self_attention(x,attention_mask)\n",
        "\n",
        "    x = self.norm1(x + self.dropout(attention_output))\n",
        "\n",
        "    ff_output = self.ff(x)\n",
        "\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "iVwfdNNObDeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, hidden_size, intermediate_size, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.decoder_layers = nn.ModuleList([\n",
        "        DecoderLayer(num_heads, hidden_size, intermediate_size, dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "  def forward(self, x, attention_mask = None):\n",
        "    for layer in self.decoder_layers:\n",
        "      x = layer(x, attention_mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "aUhEyZpibELT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_sequence_length, dropout,\n",
        "               num_layers, num_heads, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.embed = Embedding(vocab_size, hidden_size, max_sequence_length, dropout)\n",
        "    self.decoder = Decoder(num_layers, num_heads, hidden_size, intermediate_size, dropout)\n",
        "    self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
        "    self.lm_head.weight = self.embed.token_embeds.weight\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    x = self.embed(input_ids)\n",
        "    mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "    x = self.decoder(x,mask)\n",
        "    return self.lm_head(x)\n"
      ],
      "metadata": {
        "id": "Qq3l9qQtj2Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 3e-4\n",
        "betas = (0.9, 0.98)\n",
        "dropout = 0.1\n",
        "epochs = 10\n",
        "warmup_steps = 4000\n",
        "pad_token_id = 0 #to be checked if this is correct.my guess it that this depends on the\n",
        "#tokenizer that i use.\n",
        "\n",
        "\n",
        "model = GPT(\n",
        "    vocab_size= 30000, hidden_size=512,\n",
        "    max_sequence_length = 512, dropout=dropout,\n",
        "    num_layers=6, num_heads=8, intermediate_size=2048).to(device)\n",
        "\n",
        "\n",
        "#define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "#define the scheduler\n",
        "def lr_scheduler(step):\n",
        "    d_model = 512\n",
        "    return (d_model ** -0.5) * min((step + 1) ** -0.5, (step + 1) * (warmup_steps ** -1.5))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_scheduler)\n",
        "#define the loss function\n",
        "# Loss Function (Ignore pad token in loss)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n"
      ],
      "metadata": {
        "id": "fKZytld941FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Prepare Batch Data\n",
        "        input_ids = batch['input_ids'][:, :-1].to(device)        # [B, L-1]\n",
        "        attention_mask = batch['attention_mask'][:, :-1].to(device)\n",
        "        target_labels = batch['input_ids'][:, 1:].to(device)     # [B, L-1]\n",
        "        # input_ids:     what is the capital of france ?\n",
        "        # target_labels:       is the capital of france ? [EOS]\n",
        "\n",
        "        # Forward Pass\n",
        "        logits = model(input_ids, attention_mask)  # [B, L_tgt, Vocab_size]\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), target_labels.reshape(-1))  # Flatten to [B*L_tgt, Vocab_size] & [B*L_tgt]\n",
        "\n",
        "        # Backward Pass & Optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        global_step += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Checkpoint\n",
        "    torch.save(model.state_dict(), f\"transformer_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Optional: Evaluate on Validation Data\n",
        "    # evaluate(model, val_dataloader)\n",
        "\n",
        "print(\"Training Completed.\")\n"
      ],
      "metadata": {
        "id": "1FjPGm1EV3aS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea03240-ff5f-40dc-8fb7-2c189aa99637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Average Loss: 266.2534\n",
            "Epoch 2/10 | Average Loss: 266.2830\n",
            "Epoch 3/10 | Average Loss: 265.5534\n",
            "Epoch 4/10 | Average Loss: 265.8050\n",
            "Epoch 5/10 | Average Loss: 267.0935\n",
            "Epoch 6/10 | Average Loss: 266.0539\n",
            "Epoch 7/10 | Average Loss: 267.3079\n",
            "Epoch 8/10 | Average Loss: 265.7449\n",
            "Epoch 9/10 | Average Loss: 266.0015\n",
            "Epoch 10/10 | Average Loss: 269.0102\n",
            "Training Completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "f5wOOmpGp54j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to infer this model? [greedy]"
      ],
      "metadata": {
        "id": "32P5NGbkp6ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids'].to(model.lm_head.weight.device)  # [1, L]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Attention mask: 1 for non-pad tokens\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # Get logits for entire sequence\n",
        "        logits = model(input_ids, attention_mask)  # [1, L, vocab]\n",
        "\n",
        "        # Only take the logits of the last token\n",
        "        next_token_logits = logits[:, -1, :]  # [1, vocab]\n",
        "\n",
        "        # Greedy: take argmax\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)  # [1, 1]\n",
        "\n",
        "        # Append to input_ids\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "        # Optional: Break if EOS token generated\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    # Decode generated tokens to text\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "4rFl2OcoqEeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}